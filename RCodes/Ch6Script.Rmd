---
title: "Multiple Regression in $R$"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE, comment=NA, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Weâ€™ll use the `Credit` dataframe from the `ISLR` package to demonstrate multiple regression with:

A numerical outcome variable $y$, in this case credit card balance.

Two explanatory variables:

  1) A first numerical explanatory variable $x_1$. In this case, their credit limit.

  2) A second numerical explanatory variable $x_2$. In this case, their income (in thousands of dollars).

Note: This dataset is not based on actual individuals, it is a simulated dataset used for educational purposes.

Analogous to simple linear regression, where the regression function $E\{Y\} = \beta_0 + \beta_1X_1$ is
a line, regression function $E\{Y\} = \beta_0 + \beta_1X_1 +\beta_2X_2$ is a plane.

```{r comment=NA, warning=FALSE, message=FALSE}
library(ISLR)
library(plotly)

data(Credit)
head(Credit)

# draw 3D scatterplot
p <- plot_ly(data = Credit, z = ~Balance, x = ~Income, y = ~Limit, opacity = 0.6, color = Credit$Balance) %>%
  add_markers() 
p

#Fit a multiple linear regression model 
#Syntax: model_name <- lm(y ~ x1 + x2 + ... +xn, data = data_frame_name)

Balance_model <- lm(Balance ~ Limit + Income, data = Credit)
Balance_model
range(Credit$Limit)
range(Credit$Income)
```

## Scatter plot matrix for Dwaine Studios example {-}


```{r}
#DwaineData <- read.table("https://people.stat.sc.edu/Hitchcock/studiodata.txt", sep ="" , header = FALSE)

DwaineData <- read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%206%20Data%20Sets/CH06FI05.txt", sep ="" , header = FALSE)

#DwaineData <- read.table("https://people.stat.sc.edu/Hitchcock/studiodata.txt", sep ="" , header = FALSE)

DwaineData <- data.frame(DwaineData)


colnames(DwaineData) <- c("TARGTPOP", "DISPOINC", "SALES")
head(DwaineData)
dim(DwaineData)

pairs(DwaineData)


```


## Correlation matrix for Dwaine Studios example {-}

```{r}
cor(DwaineData)
```


```{r}
# Wanna make it fancy?
library("GGally")
ggpairs(DwaineData)
```


## Correlation Test for Normality


```{r}

```



## Breusch-Pagan Test for Constancy of Error Variance {-}

```{r comment=NA, warning=FALSE, message=FALSE}
DwaineModel <- lm(SALES ~ TARGTPOP + DISPOINC, data = DwaineData)

library(lmtest)
bptest(DwaineModel, student = FALSE)
```

### F test for lack of fit {-}

```{r comment=NA, warning=FALSE, message=FALSE}
# F test for lack of fit

#Reduced <- lm(SALES ~ TARGTPOP + DISPOINC, data = DwaineData) # This is our usual model
#summary(Reduced)



#Full <-  lm(SALES ~ factor(TARGTPOP)*factor(DISPOINC), data = DwaineData) # This is the full model
#summary(Full)

#anova (Reduced, Full)


```



```{r comment=NA, warning=FALSE, message=FALSE}
# Problem 5 from exercises

p5 <- read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%206%20Data%20Sets/CH06PR05.txt", sep ="" , header = FALSE)


p5Data <- data.frame(p5)


colnames(p5Data) <- c("y", "x1", "x2")
(p5Data)
dim(p5Data)


Reduced <- lm(y ~ x1 + x2, data = p5Data) # This is our usual model
summary(Reduced)

Full <-  lm(y ~ factor(x1)*factor(x2), data = p5Data) # This is the full model
summary(Full)

```

Testing the hypothesis:

$H_0: \mbox{Reduced model is good}$ $\quad$ $H_a: \mbox{Full model is good}$

```{r comment=NA, warning=FALSE, message=FALSE}

anova (Reduced, Full)


```


### F Test for Regression Relation, Problem 6.6 in the exercises

```{r comment=NA, warning=FALSE, message=FALSE}
library(Matrix)

p5 <- read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%206%20Data%20Sets/CH06PR05.txt", sep ="" , header = FALSE)


p5Data <- data.frame(p5)


colnames(p5Data) <- c("y", "x1", "x2")
#dim(p5Data)

######

n <- nrow(p5Data)


Y <- p5Data$y

Y <- as.matrix(Y) 
Y                       # This is your Y


X <- p5Data[,-1]        # remove y to form X
X <- as.matrix(X)
X                       # Not quite the X we need

X <- cbind(rep(1,n), X) # Adding a column of ones to make the X matrix
X                       # This is your X


# Calculating SSTO 

YtY <- t(Y) %*% Y
YtY

J <- matrix(1, n, n)
#J

YtJY <- t(Y) %*% J %*% Y
YtJY

SSTO <- YtY - (1/n) * YtJY
SSTO

# Calculating SSE

b <- solve(t(X) %*% X) %*% t(X) %*% Y

SSE <- YtY - t(b) %*% t(X) %*% Y
SSE

# Calculating SSR

SSR <- SSTO - SSE
SSR

# Calculating F statistic

p <- 3 # number of predictors

MSR <- SSR/(p-1)
MSE <- SSE/(n-p)

Fstat <- MSR/MSE
Fstat  

  
# P-value
pf(Fstat, p-1, n-p, lower.tail = FALSE)

```

## Varifying the above results

```{r comment=NA, warning=FALSE, message=FALSE}
p5model <- lm(y~x1+x2, data = p5Data)
summary(p5model) # Read the very last line of the output 
```

## Test for individual $\beta$'s

```{r comment=NA, warning=FALSE, message=FALSE}
p5model <- lm(y~x1+x2, data = p5Data)
summary(p5model) 
?confint


confint(p5model, method = "none")

confint(p5model, method = c("none","bonferroni"))

```



## Interval Estimation of $\beta_k$

```{r comment=NA, warning=FALSE, message=FALSE}
confint(p5model, level = .95)
```



## Prediction of New Observation $Y_{h(new)}$


```{r comment=NA, warning=FALSE, message=FALSE}
p5model <- lm(y~x1+x2, data = p5Data)

#prediction interval for the new observation x1 = 2, x2 = 4

xnew <- t(c(x1 = 2, x2 = 4))
xnew <- data.frame(xnew)
xnew

predict(p5model, xnew,interval="pred",level=.95) #prediction interval

```


## Prediction of Mean of m New Observations at $X_h$

```{r comment=NA, warning=FALSE, message=FALSE}
p5model <- lm(y~x1+x2, data = p5Data)

# prediction interval for the new observation x1 = 2, x2 = 4; x1 = 3, x2 = 3

xnew <- matrix(c(2,4,3,3), nrow = 2, byrow = TRUE)
xnew <- data.frame(xnew)
xnew

colnames(xnew) <- c("x1", "x2")
xnew

predict(p5model, xnew, interval="conf",level=.95) #confidence interval
```


