---
title:  "Buildrng the Regression Model I: Model Selection and Validation"
author: Dr. Hasthika Rupasinghe
date: "Nov 07, 2023"
output:
  rmdformats::readthedown:
    toc_depth: 3
---

```{r setup, include=FALSE, comment=NA, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Getting data

```{r comment=NA, warning=FALSE, message=FALSE}
surgicalData <- read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%209%20Data%20Sets/CH09TA01.txt", sep ="" , header = FALSE)


#Look at the first 6 entries
head(surgicalData)

names(surgicalData) <- c(paste("X", 1:8, sep = ""), "Y" , "lnY")
head(surgicalData)
```

For simplicity we only use first four explanatory variable and first 54 of the 108 patients.

## Stem and Leaf plots for individual X variables

```{r comment=NA, warning=FALSE, message=FALSE}

s1<- stem(surgicalData$X1, 3)
s2<- stem(surgicalData$X2, 3)
s3<- stem(surgicalData$X3, 3)
s4<- stem(surgicalData$X4, 3)


```

## Box plots for individual X variables
```{r}
library(ggplot2)
library(cowplot)
bp1<- ggplot(surgicalData, aes(x = X1)) + geom_boxplot(color = "steelblue", fill = "orchid4") + theme_bw()+coord_flip()
bp2<-ggplot(surgicalData, aes(x = X2)) + geom_boxplot(color = "steelblue", fill = "orchid4") + theme_bw()+coord_flip()
bp3<-ggplot(surgicalData, aes(x = X3)) + geom_boxplot(color = "steelblue", fill = "orchid4") + theme_bw()+coord_flip()
bp4<-ggplot(surgicalData, aes(x = X4)) + geom_boxplot(color = "steelblue", fill = "orchid4") + theme_bw()+coord_flip()

plot_grid(bp1, bp2,bp3,bp4)

```

## Correlation Matrix

```{r comment=NA, warning=FALSE, message=FALSE}
cor(surgicalData[1:4])

pairs(surgicalData[1:4])
```

## First order regression model

```{r comment=NA, warning=FALSE, message=FALSE}
surgicalModel1 <- lm(Y ~ X1 + X2 + X3 + X4, data = surgicalData)

library(ggplot2)
library(cowplot)
rp <- ggplot(surgicalModel1, aes(x = .fitted, y = .resid)) + geom_point(color = "blue", dotsize = .5) + theme_bw() + geom_line(y=0)

npp <- ggplot(surgicalModel1, aes(sample = .resid)) + geom_qq(color = "red") + geom_qq_line()+ theme_bw()

plot_grid(rp, npp)
```

The plot suggests that both curvature and nonconstant error variance are
apparent. In addition, some departure from normality is suggested by the normal probability
plot of residuals.

To make the distribution of the error terms more nearly normal and to see if the same
transformation would also reduce the apparent curvature, the investigator examined the logarithmic transformation $Y' = ln Y$.


## Regression model with transformed Y

```{r comment=NA, warning=FALSE, message=FALSE}
surgicalModel2 <- lm(lnY ~ X1 + X2 + X3 + X4, data = surgicalData)

library(ggplot2)
library(cowplot)
rp <- ggplot(surgicalModel2, aes(x = .fitted, y = .resid)) + geom_point(color = "blue", dotsize = .5) + theme_bw() + geom_line(y=0)

npp <- ggplot(surgicalModel2, aes(sample = .resid)) + geom_qq(color = "red") + geom_qq_line()+ theme_bw()

plot_grid(rp, npp)

```
Now the plots look better!

## Scatter Plot Matrix and Correlation Matrix when Response Variable is lnY

```{r comment=NA, warning=FALSE, message=FALSE}
cor(model.frame(surgicalModel2))
#cor(surgicalData[c(10, 1:4)])

pairs(model.frame(surgicalModel2), pch = 20, col = "darkgreen")
```
All of these plots indicate that each of the predictor variables is Linearly associated with lnY , with X3  and X4 showing the highest
degrees of association and X1 the lowest. 

The scatter plot matrix and the correlation matrix further show intercorrelations among the potential predictor variables. In particular, X4  has
moderately high pairwise correlations with __________ __________ __________ .


<!---Section 9.3--->

## Plot with $r^2$

```{r comment=NA, warning=FALSE, message=FALSE}
library(leaps)
library(dplyr)

?regsubsets #from leaps library

surgicalData
subsets <- regsubsets(x = surgicalData[1:4], y = surgicalData[,10], which = "rsq", nbest = 4) #nbest: number of subsets of each size to record

summary(subsets)

plot(subsets, scale = "r2")


### Using leaps

obj <- leaps(x = surgicalData[1:4], y = surgicalData[,10], method = c( "r2"), nbest = 4) #nbest: number of subsets of each size to record

obj


tab <- cbind(obj$size, obj$r2)
tab
tab <- data.frame(tab)
tab


names(tab) <- c("p", "r2")
tab 


tabmax <- tab %>%
  group_by(p) %>%
  summarize(mr2 = max(r2))
tabmax

ggplot(data = tab, aes(x=p, y=r2)) + geom_point(color = "red") + 
   geom_line(data = tabmax, aes(x=p, y=mr2), color = "blue") + theme_bw()

tab2 <- cbind(obj$size, obj$r2, obj$which)
tab2


```



## Plot with $R^2_{adj}$

```{r}
subsets <- regsubsets(x = surgicalData[1:4], y = surgicalData[,10], which = "adjr2", nbest = 4) #nbest: number of subsets of each size to record

summary(subsets)

plot(subsets, scale = "adjr2")


### Using leaps

obj <- leaps(x = surgicalData[1:4], y = surgicalData[,10], method = c("adjr2"), nbest = 4) # only change the method from previous criterion

obj


tab <- cbind(obj$size, obj$adjr2)
tab <- data.frame(tab)
tab

names(tab) <- c("p", "adjr2")
tab 

tabmax <- tab %>%
  group_by(p) %>%
  summarize(madjr2 = max(adjr2))
tabmax

ggplot(data = tab, aes(x=p, y=adjr2)) + geom_point(color = "red") + 
   geom_line(data = tabmax, aes(x=p, y=madjr2), color = "blue") + theme_bw()

tab2 <- cbind(obj$size, obj$adjr2, obj$which)
tab2

tab2[which.max(tab2[,2]),] # Anotherway to find the  best subset model using  adjusted r^2 criterion
```

## Plot with $C_p$

```{r}
subsets <- regsubsets(x = surgicalData[1:4], y = surgicalData[,10], which = "cp", nbest = 4) #nbest: number of subsets of each size to record

summary(subsets)

plot(subsets, scale = "Cp")


### Using leaps

obj <- leaps(x = surgicalData[1:4], y = surgicalData[,10], method = c( "Cp"), nbest = 4)

obj

tab <- cbind(obj$size, obj$Cp)

tab <- data.frame(tab)
tab

names(tab) <- c("p", "Cp")
tab 

tabmax <- tab %>%
  group_by(p) %>%
  summarize(mCp = min(Cp))
tabmax

ggplot(data = tab, aes(x=p, y=Cp)) + geom_point(color = "red") + 
   geom_line(data = tabmax, aes(x=p, y=mCp), color = "blue") + theme_bw()

tab2 <- cbind(obj$size, obj$Cp, obj$which)
tab2

tab2[which.min(tab2[,2]),]
```

## Multiple Criterians at once

```{r}
library(olsrr)

obj2 <- ols_step_all_possible(surgicalModel2)
obj2 # Default table
plot(obj2) # Plots

# Custom Tables (Ex: with AIC)
tabAIC <- cbind(obj2$predictors, obj2$aic)
tabAIC <- data.frame(tabAIC)
names(tabAIC) <- c("Predictors", "AIC")
tabAIC 

```

## Best subsets algorithms

```{r}
library(olsrr)
# To obtain the set of plots in page 362 
surgicalModelAll <- lm(lnY ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8, data = surgicalData)
summary(surgicalModelAll)

obj3 <- ols_step_all_possible(surgicalModelAll)
obj3 # Default table
plot(obj3) # Plots


# To get the table 9.3
library(leaps)
obj <- regsubsets(x = surgicalData[1:8], y = surgicalData[,10], nbest = 1, which = "adjr2")
objSummary <- summary(obj)
objSummary
objSummary$adjr2
```

```{r}
obj <- regsubsets(x = surgicalData[1:8], y = surgicalData[,10], nbest = 1, which = "cp")
objSummary <- summary(obj)
objSummary
objSummary$cp
```



```{r}
obj <- regsubsets(x = surgicalData[1:8], y = surgicalData[,10], nbest = 1, which = "rss")
objSummary <- summary(obj)
objSummary
objSummary$rss
```

## Forward Stepwise Regression

```{r}
fit <- lm(lnY ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8, data = surgicalData)  # Full Model
o <- step(fit, direction = "forward")     
summary(o)
```

## Model Validation


```{r}
# Get the Validation set
surgicalDataValidation <- read.table("http://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%209%20Data%20Sets/CH09TA05.txt", sep ="" , header = FALSE)

names(surgicalDataValidation) <- c(paste("X", 1:8, sep = ""), "Y" , "lnY")

dim(surgicalDataValidation)

#------------------------------------------------------------------------------------
# We selected X1, X2, X3, X6, X8 using Cp criterion in the earlier section 
lmCpVal <- lm(lnY ~ X1 + X2 + X3 + X6 + X8, data = surgicalDataValidation)

summary(lmCpVal)

MSPR1 <- summary(lmCpVal)$sigma^2 # Getting the MSPR - MSE for the Validation(prediction) set
MSPR1
adjRsq1 <- summary(lmCpVal)$adj.r.squared
adjRsq1

#names(summary(lmCpVal))

#------------------------------------------------------------------------------------
# We selected X1, X2, X3, X5, X6, X8 using R2adj criterion in the earlier section 

lmRVal <- lm(lnY ~ X1 + X2 + X3 + X5 + X6 + X8, data = surgicalDataValidation)

summary(lmRVal)

MSPR2 <- summary(lmRVal)$sigma^2 # Getting the MSPR - MSE for the Validation(prediction) set
MSPR2
adjRsq2 <- summary(lmRVal)$adj.r.squared
adjRsq2

#------------------------------------------------------------------------------------
# Making a table so it is easier to compre the models

compare <- data.frame(matrix(c(MSPR1, MSPR2, adjRsq1, adjRsq2), ncol = 2, byrow = TRUE))
colnames(compare) <- c( "Model 1" , "Model 2")
rownames(compare) <- c( "MSPR" , "Adj R square")
compare
```


